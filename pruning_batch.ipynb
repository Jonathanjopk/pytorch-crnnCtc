{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from src.model import CRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = CRNN(img_channel=32, img_height=128, img_width=128, num_class=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu0): ReLU(inplace)\n",
       "  (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU(inplace)\n",
       "  (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU(inplace)\n",
       "  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU(inplace)\n",
       "  (pooling2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4): ReLU(inplace)\n",
       "  (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu5): ReLU(inplace)\n",
       "  (pooling3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu6): ReLU(inplace)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33, 66, 139, 145, 257, 232, 247]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 剪枝比率\n",
    "pruning_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cfg\n",
    "'''\n",
    "model: 各架構位置\n",
    "skip: 不剪枝的層數\n",
    "cfg: 剪枝後剩餘的 channel 數量\n",
    "cfg_mask: 剪枝後剩餘 channel 的位置\n",
    "cat_layer: 有 concat 的層數\n",
    "'''\n",
    "pruning_cfg = {\n",
    "    'cnn':{\n",
    "        'model': model.cnn,\n",
    "        'skip': [],\n",
    "        'cfg': [],\n",
    "        'cfg_mask': [],\n",
    "        'cat_layer': []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global threshold: 0.50532466173172\n",
      "Total channels: 2240\n"
     ]
    }
   ],
   "source": [
    "\"\"\"計算global threshold\"\"\"\n",
    "# 計算總共多少 channels\n",
    "total = 0\n",
    "for m in model.cnn.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        total += m.weight.data.shape[0] # m.weight 就是 gamma\n",
    "\n",
    "# 所有 gamma 值 取絕對值存進 bn\n",
    "bn = torch.zeros(total) # 1*n維\n",
    "index = 0\n",
    "for m in model.cnn.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        size = m.weight.data.shape[0] # channels\n",
    "        bn[index:(index + size)] = m.weight.data.abs().clone()\n",
    "        index += size\n",
    "# 由小到大排序\n",
    "y, i = torch.sort(bn) # 小 -> 大\n",
    "thre_index = int(total * pruning_rate) # scale sparse rate 0.5 剪枝比例\n",
    "thre = y[thre_index] if thre_index != 0 else 0 # 取第 thre_index 個值當作 threshold，如果 thre_index=0 代表全留，不能取第 0 個要直接改 0\n",
    "# 之後 weight 會跟 thre 這個數字比大小，產生一個 0, 1 的 tensor，大於 thre 的留下(小於 thre 的就不會被存進 newmodel)\n",
    "print('Global threshold: {}'.format(thre))\n",
    "print('Total channels: {}'.format(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(改變) !!!!!!! 要先有第一層 image channel !!!!!!! <br>\n",
    "起初的 image_channel 皆保留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer index: 2 \t total channel: 64 \t remaining channel: 33\n",
      "layer index: 6 \t total channel: 128 \t remaining channel: 66\n",
      "layer index: 10 \t total channel: 256 \t remaining channel: 139\n",
      "layer index: 13 \t total channel: 256 \t remaining channel: 145\n",
      "layer index: 17 \t total channel: 512 \t remaining channel: 257\n",
      "layer index: 20 \t total channel: 512 \t remaining channel: 232\n",
      "layer index: 24 \t total channel: 512 \t remaining channel: 247\n",
      "-------------------------------------------------------------------------\n",
      "channels pruned / channels total: 1121.0 / 2240\n",
      "pruned ratio: 0.5004464387893677\n"
     ]
    }
   ],
   "source": [
    "\"\"\"記錄誰該留下誰該剪掉\"\"\"\n",
    "pruned = 0\n",
    "cfg_new = [32] # remaining channel\n",
    "cfg_mask = [torch.ones(32)] # 記錄每層 channels，以 0,1 表示剪枝，假設 channels=3, cfg_mask=[0,1,1]\n",
    "for k, m in enumerate(model.cnn.modules()):\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        thre_ = 0 if k in pruning_cfg['cnn']['skip'] else thre # skip 的 layer thre=0\n",
    "        weight_copy = m.weight.data.abs().clone()\n",
    "        mask = weight_copy.gt(thre_).float() # 比大小，大的標記 1 & 小的標記 0，存進 mask\n",
    "\n",
    "        cfg_new.append(int(torch.sum(mask)))\n",
    "        cfg_mask.append(mask.clone())\n",
    "\n",
    "        pruned = pruned + mask.shape[0] - torch.sum(mask) # 計算pruning ratio\n",
    "        print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.\n",
    "            format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "pruned_ratio = pruned / total\n",
    "print('-------------------------------------------------------------------------')\n",
    "print('channels pruned / channels total: {} / {}'.format(pruned, total))\n",
    "print('pruned ratio: {}'.format(pruned_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 33, 66, 139, 145, 257, 232, 247]\n"
     ]
    }
   ],
   "source": [
    "print(cfg_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cfg_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save weights to new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cfg\n",
    "'''\n",
    "model: 各架構位置\n",
    "skip: 不剪枝的層數\n",
    "cfg: 剪枝後剩餘的 channel 數量\n",
    "cfg_mask: 剪枝後剩餘 channel 的位置\n",
    "cat_layer: 有 concat 的層數\n",
    "'''\n",
    "pruning_cfg = {\n",
    "    'cnn':{\n",
    "        'model': model.cnn,\n",
    "        'skip': [],\n",
    "        'cfg': [32, 33, 66, 139, 145, 257, 232, 247],\n",
    "        'cfg_mask': [],\n",
    "        'cat_layer': []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 用新的 cfg 定義新模型架構\n",
    "newmodel = CRNN(img_channel=32, img_height=128, img_width=128, num_class=5, pruning_cfg=pruning_cfg['cnn']['cfg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv0): Conv2d(32, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm0): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu0): ReLU(inplace)\n",
       "  (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1): Conv2d(33, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm1): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU(inplace)\n",
       "  (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(66, 139, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm2): BatchNorm2d(139, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU(inplace)\n",
       "  (conv3): Conv2d(139, 145, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm3): BatchNorm2d(145, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU(inplace)\n",
       "  (pooling2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(145, 257, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm4): BatchNorm2d(257, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4): ReLU(inplace)\n",
       "  (conv5): Conv2d(257, 232, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm5): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu5): ReLU(inplace)\n",
       "  (pooling3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv6): Conv2d(232, 247, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (batchnorm6): BatchNorm2d(247, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu6): ReLU(inplace)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel.cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_modules = list(model.cnn.modules())\n",
    "new_modules = list(newmodel.cnn.modules())\n",
    "layer_id_in_cfg = 0\n",
    "start_mask = cfg_mask[layer_id_in_cfg] # 第一個維度\n",
    "end_mask = cfg_mask[layer_id_in_cfg+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(改變) 先做 conv 在做 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "In shape: 32, Out shape 33.\n",
      "=====================================================\n",
      "In shape: 33, Out shape 66.\n",
      "=====================================================\n",
      "In shape: 66, Out shape 139.\n",
      "=====================================================\n",
      "In shape: 139, Out shape 145.\n",
      "=====================================================\n",
      "In shape: 145, Out shape 257.\n",
      "=====================================================\n",
      "In shape: 257, Out shape 232.\n",
      "=====================================================\n",
      "In shape: 232, Out shape 247.\n"
     ]
    }
   ],
   "source": [
    "for layer_id in range(len(old_modules)):\n",
    "    m0 = old_modules[layer_id]\n",
    "    m1 = new_modules[layer_id]\n",
    "\n",
    "    # 針對 conv\n",
    "    if isinstance(m0, nn.Conv2d):\n",
    "        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
    "        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
    "        print('=====================================================')\n",
    "        print('In shape: {:d}, Out shape {:d}.'.format(idx0.size, idx1.size))\n",
    "        if idx0.size == 1:\n",
    "            idx0 = np.resize(idx0, (1,))\n",
    "        if idx1.size == 1:\n",
    "            idx1 = np.resize(idx1, (1,))\n",
    "        w1 = m0.weight.data[:, idx0.tolist(), :, :].clone() # in_channel\n",
    "        w1 = w1[idx1.tolist(), :, :, :].clone() # out_channel\n",
    "        m1.weight.data = w1.clone() # 存入新的權重\n",
    "\n",
    "    # 針對 batchnorm\n",
    "    elif isinstance(m0, nn.BatchNorm2d):\n",
    "        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
    "        if idx1.size == 1:\n",
    "            idx1 = np.resize(idx1, (1,))\n",
    "\n",
    "        # 存入新的權重\n",
    "        m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n",
    "        m1.bias.data = m0.bias.data[idx1.tolist()].clone()\n",
    "        m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n",
    "        m1.running_var = m0.running_var[idx1.tolist()].clone()\n",
    "        \n",
    "        # 跑最後一層會有 list index 超出範圍，所以限制\n",
    "        if layer_id_in_cfg < 6:\n",
    "            layer_id_in_cfg += 1\n",
    "            start_mask = end_mask.clone()\n",
    "            end_mask = cfg_mask[layer_id_in_cfg+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= conv0 =================\n",
      "Conv shape: torch.Size([33, 32, 3, 3])\n",
      "Batch shape: torch.Size([33])\n",
      "================= conv1 =================\n",
      "Conv shape: torch.Size([66, 33, 3, 3])\n",
      "Batch shape: torch.Size([66])\n",
      "================= conv2 =================\n",
      "Conv shape: torch.Size([139, 66, 3, 3])\n",
      "Batch shape: torch.Size([139])\n",
      "================= conv3 =================\n",
      "Conv shape: torch.Size([145, 139, 3, 3])\n",
      "Batch shape: torch.Size([145])\n",
      "================= conv4 =================\n",
      "Conv shape: torch.Size([257, 145, 3, 3])\n",
      "Batch shape: torch.Size([257])\n",
      "================= conv5 =================\n",
      "Conv shape: torch.Size([232, 257, 3, 3])\n",
      "Batch shape: torch.Size([232])\n",
      "================= conv6 =================\n",
      "Conv shape: torch.Size([247, 232, 2, 2])\n",
      "Batch shape: torch.Size([247])\n"
     ]
    }
   ],
   "source": [
    "# 新的 model\n",
    "# torch.size() 順序是 output channel,  input channel, kernel size\n",
    "for i in newmodel.cnn.state_dict():\n",
    "    if ('conv' in i) and ('weight' in i):\n",
    "        print((\"================= {} =================\").format(i.split('.')[0]))\n",
    "        print('Conv shape: {}'.format(newmodel.cnn.state_dict()[i].shape))\n",
    "    if ('batchnorm' in i) and ('weight' in i):\n",
    "        print('Batch shape: {}'.format(newmodel.cnn.state_dict()[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= conv0 =================\n",
      "Conv shape: torch.Size([64, 32, 3, 3])\n",
      "Batch shape: torch.Size([64])\n",
      "================= conv1 =================\n",
      "Conv shape: torch.Size([128, 64, 3, 3])\n",
      "Batch shape: torch.Size([128])\n",
      "================= conv2 =================\n",
      "Conv shape: torch.Size([256, 128, 3, 3])\n",
      "Batch shape: torch.Size([256])\n",
      "================= conv3 =================\n",
      "Conv shape: torch.Size([256, 256, 3, 3])\n",
      "Batch shape: torch.Size([256])\n",
      "================= conv4 =================\n",
      "Conv shape: torch.Size([512, 256, 3, 3])\n",
      "Batch shape: torch.Size([512])\n",
      "================= conv5 =================\n",
      "Conv shape: torch.Size([512, 512, 3, 3])\n",
      "Batch shape: torch.Size([512])\n",
      "================= conv6 =================\n",
      "Conv shape: torch.Size([512, 512, 2, 2])\n",
      "Batch shape: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# 新的 model\n",
    "# torch.size() 順序是 output channel,  input channel, kernel size\n",
    "for i in model.cnn.state_dict():\n",
    "    if ('conv' in i) and ('weight' in i):\n",
    "        print((\"================= {} =================\").format(i.split('.')[0]))\n",
    "        print('Conv shape: {}'.format(model.cnn.state_dict()[i].shape))\n",
    "    if ('batchnorm' in i) and ('weight' in i):\n",
    "        print('Batch shape: {}'.format(model.cnn.state_dict()[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
